{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/17 17:35~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정적 웹 크롤링과 스크래핑을 이용한 공공데이터와 SNS 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'서울': ['9~18', '9~18', '7~15', '7~15', '5~13', '5~13', '5~13', '5~13', '4~12', '4~12', '5~11', '4~11', '2~9'], '인천': ['10~17', '10~17', '8~14', '8~14', '6~12', '6~12', '6~12', '6~12', '5~11', '5~11', '6~11', '5~11', '4~9'], '수원': ['8~18', '8~18', '6~16', '6~16', '4~13', '4~13', '5~14', '5~14', '3~12', '3~12', '4~12', '4~11', '1~10'], '파주': ['5~17', '5~17', '3~14', '3~14', '0~13', '0~13', '1~12', '1~12', '1~11', '1~11', '2~11', '0~10', '-2~8'], '이천': ['5~18', '5~18', '3~16', '3~16', '2~14', '2~14', '3~13', '3~13', '1~12', '1~12', '0~12', '0~10', '-1~9'], '평택': ['7~19', '7~19', '5~17', '5~17', '4~16', '4~16', '7~15', '7~15', '4~13', '4~13', '2~13', '3~12', '2~12'], '춘천': ['5~17', '5~17', '4~15', '4~15', '2~13', '2~13', '3~13', '3~13', '2~11', '2~11', '1~11', '1~9', '-1~8'], '원주': ['6~17', '6~17', '5~15', '5~15', '4~14', '4~14', '5~13', '5~13', '2~12', '2~12', '2~12', '2~10', '1~9'], '강릉': ['10~18', '10~18', '10~18', '10~18', '9~14', '9~14', '8~16', '8~16', '7~14', '7~14', '6~14', '7~14', '4~13'], '대전': ['8~19', '8~19', '5~17', '5~17', '5~15', '5~15', '6~15', '6~15', '4~13', '4~13', '2~13', '4~12', '2~11'], '세종': ['7~19', '7~19', '5~17', '5~17', '4~15', '4~15', '6~15', '6~15', '4~12', '4~12', '2~12', '3~12', '1~11'], '홍성': ['7~18', '7~18', '5~16', '5~16', '4~15', '4~15', '6~15', '6~15', '4~12', '4~12', '2~13', '3~12', '2~11'], '청주': ['8~18', '8~18', '6~16', '6~16', '5~14', '5~14', '5~15', '5~15', '4~13', '4~13', '3~13', '4~11', '1~10'], '충주': ['6~18', '6~18', '3~16', '3~16', '4~14', '4~14', '4~14', '4~14', '2~12', '2~12', '1~12', '1~10', '-1~10'], '영동': ['6~18', '6~18', '2~17', '2~17', '3~16', '3~16', '5~15', '5~15', '2~13', '2~13', '1~13', '2~12', '0~14'], '광주': ['10~20', '10~20', '7~19', '7~19', '8~17', '8~17', '7~16', '7~16', '5~14', '5~14', '5~15', '5~13', '4~13'], '목포': ['11~17', '11~17', '9~17', '9~17', '9~15', '9~15', '8~15', '8~15', '8~14', '8~14', '6~14', '8~13', '6~13'], '여수': ['13~18', '13~18', '11~18', '11~18', '11~16', '11~16', '10~16', '10~16', '9~15', '9~15', '8~14', '8~14', '7~13'], '순천': ['9~20', '9~20', '7~20', '7~20', '8~18', '8~18', '9~17', '9~17', '7~17', '7~17', '5~16', '6~15', '4~15'], '광양': ['11~20', '11~20', '8~20', '8~20', '9~18', '9~18', '9~16', '9~16', '6~16', '6~16', '6~16', '7~15', '4~13'], '나주': ['7~20', '7~20', '4~19', '4~19', '5~18', '5~18', '6~17', '6~17', '5~15', '5~15', '3~14', '4~14', '4~15'], '전주': ['8~19', '8~19', '6~18', '6~18', '7~16', '7~16', '6~15', '6~15', '5~14', '5~14', '4~14', '5~13', '3~12'], '군산': ['8~19', '8~19', '6~17', '6~17', '7~15', '7~15', '7~14', '7~14', '5~13', '5~13', '3~14', '5~12', '2~12'], '정읍': ['8~18', '8~18', '6~17', '6~17', '6~15', '6~15', '5~14', '5~14', '4~13', '4~13', '3~14', '4~13', '2~12'], '남원': ['7~19', '7~19', '4~19', '4~19', '5~17', '5~17', '6~16', '6~16', '4~14', '4~14', '2~14', '4~13', '2~13'], '고창': ['8~19', '8~19', '6~18', '6~18', '6~16', '6~16', '6~15', '6~15', '5~13', '5~13', '4~14', '5~13', '2~12'], '무주': ['6~18', '6~18', '3~16', '3~16', '3~16', '3~16', '5~15', '5~15', '2~13', '2~13', '1~13', '2~12', '1~13'], '부산': ['13~20', '13~20', '11~21', '11~21', '11~19', '11~19', '11~18', '11~18', '9~17', '9~17', '8~16', '9~15', '7~15'], '울산': ['11~18', '11~18', '10~19', '10~19', '9~16', '9~16', '9~16', '9~16', '7~16', '7~16', '6~16', '6~15', '4~14'], '창원': ['10~18', '10~18', '9~18', '9~18', '9~17', '9~17', '8~16', '8~16', '7~16', '7~16', '6~15', '6~14', '4~14'], '진주': ['8~19', '8~19', '5~19', '5~19', '5~17', '5~17', '6~17', '6~17', '4~16', '4~16', '2~16', '3~15', '1~14'], '거창': ['5~18', '5~18', '2~18', '2~18', '3~16', '3~16', '3~15', '3~15', '1~14', '1~14', '0~14', '2~13', '-1~13'], '통영': ['12~19', '12~19', '11~20', '11~20', '10~18', '10~18', '9~17', '9~17', '8~16', '8~16', '7~16', '8~15', '6~15'], '대구': ['9~19', '9~19', '7~18', '7~18', '7~16', '7~16', '6~16', '6~16', '5~15', '5~15', '4~15', '4~14', '2~13'], '안동': ['7~17', '7~17', '4~17', '4~17', '4~14', '4~14', '5~15', '5~15', '2~13', '2~13', '1~13', '2~11', '0~11'], '포항': ['11~18', '11~18', '9~19', '9~19', '10~16', '10~16', '9~16', '9~16', '7~15', '7~15', '6~15', '7~14', '5~14'], '경주': ['9~19', '9~19', '6~20', '6~20', '7~17', '7~17', '7~17', '7~17', '4~15', '4~15', '3~15', '4~14', '0~12'], '울진': ['9~18', '9~18', '8~18', '8~18', '8~14', '8~14', '8~15', '8~15', '6~14', '6~14', '5~14', '6~14', '4~13'], '울릉도': ['11~16', '11~16', '11~17', '11~17', '9~13', '9~13', '9~14', '9~14', '9~13', '9~13', '9~13', '8~13', '8~12'], '제주': ['15~19', '15~19', '14~19', '14~19', '12~17', '12~17', '12~16', '12~16', '11~16', '11~16', '11~16', '11~15', '9~16'], '서귀포': ['16~20', '16~20', '14~21', '14~21', '13~19', '13~19', '13~18', '13~18', '12~18', '12~18', '11~17', '11~17', '10~17']}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108\"\n",
    "savename = \"C:/Temp/forecast.xml\" #xml파일로 저장\n",
    "req.urlretrieve(url, savename) #urlretrieve()는 받아온것을 파일로 저장하겠다는 함수.\n",
    "\n",
    "xml = open(savename, \"r\", encoding=\"utf-8\").read() #read()는 아규먼트 없이 호출하면 모두 읽어라가 된다.\n",
    "soup = BeautifulSoup(xml, 'xml') # xml #xml문서니까 2번째 argument를 'xml'로 지정한다. 2번째 argument는 Parser를 지정하는 것이다. #xml은 xml파서로, html파일은 html파서로 하는 게 좋다.\n",
    "\n",
    "info = {}\n",
    "for location in soup.find_all(\"location\"):\n",
    "    loc = location.find('city').string #(for문을 이용해 for문에서 생성한)태그 객체를 이용하면 태그객체의 자손 태그에서 찾는다. 여기선 location태그객체의 자손에서  찾는다.\n",
    "    min_w = location.find_all('tmn')\n",
    "    max_w = location.find_all('tmx')\n",
    "    weather = [a.string+\"~\"+b.string for a, b in zip(min_w, max_w)] #리스트 컴프리헨션. 대괄호 안에다가 식을 준다.\n",
    "\n",
    "    if not (loc in info):\n",
    "        info[loc] = []\n",
    "    for data in weather:\n",
    "        info[loc].append(data)\n",
    "print(info)\n",
    "\n",
    "with open('C:/Temp/forecast.txt', \"wt\", encoding=\"utf-8\") as f:\n",
    "    for loc in sorted(info.keys()): #키(도시이름)을 가지고 소팅을 하고 있다.(도시이름을 가나다순으로)\n",
    "        f.write(str(loc)+'\\n') 도시이름을 쓰고 다음 행에다가 리스트 정보를 쓴다.\n",
    "        for name in info[loc]:\n",
    "            f.write('\\t'+str(name)+'\\n') #탭하고 도시마다의 최저,최고온도를 적고 엔터키. (숫자 데이터를 하나를 한 행에 적는다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "nowDate = now.strftime('%Y년 %m월 %d일 %H시 %M분 입니다.')\n",
    "print(nowDate)\n",
    "# 오늘의 날씨\n",
    "print('[ 오늘의 날씨 요약 ]')\n",
    "webpage = urllib.request.urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query='+urllib.parse.quote_plus('서울날씨')) #https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query=서울날씨 하면 서울날씨가 나온다.\n",
    "soup = BeautifulSoup(webpage, 'html.parser') #BeautifulSoup 객체를 생성해서 파서를 적용 #심지어 내용을 읽어서 주지 않아도 알아서 BeautifulSoup 객체가 알아서 읽는다.\n",
    "temp = soup.select('div.temperature_text > strong')\n",
    "cast = soup.select('span.weather.before_slash')\n",
    "print(f'--> 서울 날씨 : {temp[0].contents[1]}℃ {cast[0].text}')\n",
    "\n",
    "webpage = urllib.request.urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query='+urllib.parse.quote_plus('광주날씨'))\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "temp = soup.select('div.temperature_text > strong')\n",
    "cast = soup.select('span.weather.before_slash')\n",
    "print(f'--> 광주 날씨 : {temp[0].contents[1]}℃ {cast[0].text}')\n",
    "\n",
    "webpage = urllib.request.urlopen('https://search.naver.com/search.naver?sm=top_hty&fbm=0&ie=utf8&query='+urllib.parse.quote_plus('제주도날씨'))\n",
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "temp = soup.select('div.temperature_text > strong')\n",
    "cast = soup.select('span.weather.before_slash')\n",
    "print(f'--> 제주도 날씨 : {temp[0].contents[1]}℃ {cast[0].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "busNum = '360'\n",
    "key = '%2BjzsSyNtwmcqxUsGnflvs3rW2oceFvhHR8AFkM3ao%2Fw50hwHXgGyPVutXw04uAXvrkoWgkoScvvhlH7jgD4%2FRQ%3D%3D'\n",
    "url1 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?serviceKey='+key+'&strSrch='+busNum\n",
    "res = req.urlopen(url1)\n",
    "\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), \"xml\")\n",
    "\n",
    "busRouteId = None\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    busRouteId = itemList.find('busRouteId').string\n",
    "    busRouteNm = itemList.find('busRouteNm').string\n",
    "    if busRouteNm == busNum :\n",
    "        break\n",
    "\n",
    "url2 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey='+key+'&busRouteId='+busRouteId\n",
    "res = req.urlopen(url2)\n",
    "soup = BeautifulSoup(res.read(), \"xml\")\n",
    "\n",
    "busPos = []\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    gpsY = itemList.find('gpsY').string\n",
    "    gpsX = itemList.find('gpsX').string\n",
    "\n",
    "    busPos.append((gpsY, gpsX))\n",
    "\n",
    "print('[ ' + busNum + '번 버스의 운행 위치 ]')\n",
    "if len(busPos) != 0 :\n",
    "    print(busNum + '번 버스 ' + str(len(busPos)) + '대 운행중...')\n",
    "    for lat,lng in busPos :\n",
    "        print(lat+','+lng)\n",
    "else :\n",
    "    print('현재 운행중인 ' + busNum + '번 버스가 없어요...')\n",
    "\n",
    "print(url1)\n",
    "print(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/LampScpgmtb/'+startIndex+'/'+endIndex+'/'\n",
    "res = req.urlopen(url)\n",
    "xml = res.read()\n",
    "soup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "pjList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    up_nm = itemList.find('UP_NM').string\n",
    "    up_nm = '없음' if up_nm is None else up_nm\n",
    "    pgm_nm = itemList.find('PGM_NM').string\n",
    "    pgm_nm = '없음' if pgm_nm is None else pgm_nm\n",
    "    target_nm = itemList.find('TARGET_NM').string\n",
    "    target_nm = '없음' if target_nm is None else target_nm\n",
    "    u_price = itemList.find('U_PRICE').string\n",
    "    u_price = '없음' if u_price is None else u_price\n",
    "    pjList.append((up_nm, pgm_nm, target_nm, u_price))\n",
    "\n",
    "print('[ 서울 청소년 수련관 강좌 리스트 ]')\n",
    "for up_nm,pgm_nm,target_nm,u_price in pjList :\n",
    "    print(up_nm+','+pgm_nm+','+target_nm+','+str(u_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "date = '20220815'\n",
    "\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/CardSubwayStatsNew/'+startIndex+'/'+endIndex+'/'+date+'/'\n",
    "print(url)\n",
    "res = req.urlopen(url)\n",
    "xml = res.read()\n",
    "\n",
    "soup = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "subwayList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    line_num = itemList.find('LINE_NUM').string\n",
    "    sub_sta_nm = itemList.find('SUB_STA_NM').string\n",
    "    ride_pasgr_num = itemList.find('RIDE_PASGR_NUM').string\n",
    "    alight_pasgr_num = itemList.find('ALIGHT_PASGR_NUM').string\n",
    "    subwayList.append((line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num))\n",
    "\n",
    "print('[ 서울시 지하철호선별 역별 승하차 인원 정보 ]')\n",
    "for line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num in subwayList :\n",
    "    print(line_num+','+sub_sta_nm+','+ride_pasgr_num+','+alight_pasgr_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '스마트폰'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "    for count, data in enumerate(dataList['items'], 1) :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList # JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList[\"items\"][0][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "desc = []\n",
    "for data in dataList[\"items\"]:\n",
    "    title.append(data[\"title\"])\n",
    "    desc.append(data[\"description\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(title)) :\n",
    "   title[i] = re.sub(\"</?b>\", \"\", title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(desc)) :\n",
    "   desc[i] = re.sub(\"</?b>\", \"\", desc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictdata = {\"title\":title,\"description\":desc}\n",
    "df = pd.DataFrame(dictdata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output/smartphone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '오징어게임'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eduvenv",
   "language": "python",
   "name": "eduvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
